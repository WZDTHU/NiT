<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="description"
        content="NiT: Native-resolution diffution Transformer for arbitrary resolution generalization" />
    <meta name="keywords"
        content="Native-Resolution Modeling, Image Synthesis" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Native-Resolution Image Synthesis</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="./css/bulma.min.css" />
    <link rel="stylesheet" href="./css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./css/fontawesome.all.min.css" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="./css/index.css" />
    <!-- add page icon at 64 x 64-->
    <!-- <link rel="icon" type="image/png" href="./assets/re.png" sizes="256x256" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./js/fontawesome.all.min.js"></script>
    <script src="./js/bulma-carousel.min.js"></script>
    <script src="./js/bulma-slider.min.js"></script>
    <script src="./js/gallery.js"></script>
    <script src="./js/index.js"></script>
</head>

<body>
    <section class="hero fade-in">
        <div class="hero-body">
            <div class="container">
                <div class="has-text-centered">
                    <h1 class="publication-title">
                        <span class="dnerf">Native-Resolution Image Synthesis</span>
                    </h1>
                    <!-- <h2 class="publication-subtitle">
                        Procedural Environment Generation and Hybrid Verifiers for Scaling Open-Weights SWE Agents
                    </h2> -->

                    <div class="publication-authors">
                        <span class="author-block">
                            <a href="https://github.com/WZDTHU">Zidong Wang</a><sup>1,2</sup>,</span>
                        <span class="author-block">
                            <a href="http://leibai.site">Lei Bai</a><sup>2,*</sup>,</span>
                        <span class="author-block">
                            <a href="https://xyue.io">Xiangyu Yue</a><sup>1</sup>,</span>
                        <span class="author-block">
                            <a href="https://wlouyang.github.io">Wanli Ouyang</a><sup>1,2</sup>,</span>
                        <span class="author-block">
                            <a href="https://invictus717.github.io">Yiyuan Zhang</a><sup>1,*</sup>,</span>
                    </div>

                    <div class="publication-authors">
                        <span class="author-block"><sup>1</sup>MMLab CUHK</span>
                        <span class="author-block"><sup>2</sup>Shanghai AI Lab</span>
                    </div>

                    <div style="margin-top: 0.5rem; font-size: 0.9rem; color: var(--text-light);">
                        <span><sup>*</sup>Correspondance</span>
                    </div>

                    <div class="publication-links">
                        <span class="link-block">
                            <a href="./assets/paper.pdf" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="#" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                <span>arXiv (upcoming)</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://github.com/WZDTHU/NiT" class="external-link button is-dark">
                                <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/datasets/GoodEnough/NiT-Preprocessed-ImageNet1K" class="external-link button is-dark">
                                <!-- <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span> -->
                                <span>ü§ó Dataset</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://huggingface.co/GoodEnough/NiT-Models" class="external-link button is-dark">
                                <!-- <span class="icon">
                                    <i class="fab fa-github"></i>
                                </span> -->
                                <span>ü§ó Models</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8"
                    style="display: flex; flex-direction: column; justify-content: center;">
                    <img src="./assets/nit/teaser.pdf" alt="Synthetic Data Generation" />
                    <p class="figure-caption">
                        <b>Figure 1:</b> A single NiT model can generate images across diverse, arbitrary
                        resolutions (from 256x256 to 2048x2048) and aspect ratios (from 1:5 to 3:1).
                    </p>
                </div>
            </div>

            <div class="columns is-centered">
                <div class="column is-10">
                    <div class="columns is-centered">
                        <div class="column is-10"
                            style="display: flex; flex-direction: column; justify-content: center;">
                            <img src="./assets/nit/comparison.pdf" alt="Synthetic Data Generation"
                                style="object-fit: contain; height: 320px;" />
                        </div>
                    </div>
                    <p class="figure-caption">
                        <b>Figure 2:</b> (a) ImageNet resolutions are mainly concentrated between 200 to 600 pixels (width/height), 
                        with sparse data beyond 800 pixels. Despite this, (b) shows our NiT's superior generalization 
                        to unseen high resolutions (e.g., 1024, 1536), evidenced by significantly lower FID scores. 
                        (c) further confirms NiT also exhibits the strongest generalization across various aspect ratios.
                    </p>
                </div>
            </div>

            <div class="columns is-centered">
                <div class="column is-10">
                    <div class="highlight-box"
                        style="background: linear-gradient(to right, #f8fafc, #f1f5f9); border-radius: 12px; border-left: 6px solid var(--primary-color); box-shadow: 0 4px 12px rgba(0,0,0,0.05);">
                        <h3 class="title is-4 has-text-centered"
                            style="margin-bottom: 1.5rem; color: var(--primary-color);">üöÄ Key Contributions </h3>

                        <div style="display: flex; flex-direction: column; gap: 1.2rem;">
                            <div class="innovation-item" style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: var(--primary-color); color: white; padding: 0.6rem; border-radius: 50%; font-size: 1.2rem; display: flex; align-items: center; justify-content: center; min-width: 2.6rem; height: 2.6rem;">
                                    üîÆ
                                </div>
                                <div>
                                    <h4 style="font-weight: 600; margin-bottom: 0.2rem; font-size: 1.1rem;">
                                        New visual generative paradigm
                                    </h4>
                                    <p style="margin: 0; line-height: 1.4;">
                                        Introduce native-resolution image synthesis, a novel generative modeling 
                                        paradigm capable of synthesizing images at arbitrary resolutions and aspect ratios.
                                    </p>
                                </div>
                            </div>

                            <div class="innovation-item" style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: var(--primary-color); color: white; padding: 0.6rem; border-radius: 50%; font-size: 1.2rem; display: flex; align-items: center; justify-content: center; min-width: 2.6rem; height: 2.6rem;">
                                    üîç
                                </div>
                                <div>
                                    <h4 style="font-weight: 600; margin-bottom: 0.2rem; font-size: 1.1rem;">
                                        Native-resolution Modeling Architecture
                                    </h4>
                                    <p style="margin: 0; line-height: 1.4;">
                                        Propose Native-resolution diffusion Transformer (NiT), an architecture designed for 
                                        explicitly modeling varying resolutions and aspect ratios within its denoising process.
                                    </p>
                                </div>
                            </div>

                            <div class="innovation-item" style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: var(--primary-color); color: white; padding: 0.6rem; border-radius: 50%; font-size: 1.2rem; display: flex; align-items: center; justify-content: center; min-width: 2.6rem; height: 2.6rem;">
                                    üèÜ
                                </div>
                                <div>
                                    <h4 style="font-weight: 600; margin-bottom: 0.2rem; font-size: 1.1rem;">
                                        State-of-the-art Performance
                                    </h4>
                                    <p style="margin: 0; line-height: 1.4;">
                                        A single NiT model simultaneously achieves the SOTA performance on both ImageNet-256x256 (2.06 FID) and 512x512 (<b>1.48 FID</b>) benchmarks.
                                        NiT highlights its strong zero-shot generalization ability (e.g., 4.52 FID on unseen 1024x1024 resolution). 
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section fade-in">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-12">
                    <h2 class="title is-3">Qualitative Showcase</h2>
                    <div class="horizontal-image-gallery">
                        <button class="gallery-arrow prev-arrow" aria-label="‰∏ä‰∏ÄÁªÑÂõæÁâá">&lt;</button>
                        <div class="gallery-viewport">
                            <div class="gallery-track">
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_1.pdf" alt="ÂõæÁâá1">
                                    <p class="gallery-item-caption">Class-33</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_2.pdf" alt="ÂõæÁâá2">
                                    <p class="gallery-item-caption">Class-88</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_3.pdf" alt="ÂõæÁâá3">
                                    <p class="gallery-item-caption">Class-250</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_4.pdf" alt="ÂõæÁâá4">
                                    <p class="gallery-item-caption">Class-279</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_5.pdf" alt="ÂõæÁâá5">
                                    <p class="gallery-item-caption">Class-388</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_6.pdf" alt="ÂõæÁâá6">
                                    <p class="gallery-item-caption">Class-417</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_7.pdf" alt="ÂõæÁâá7">
                                    <p class="gallery-item-caption">Class-437</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_8.pdf" alt="ÂõæÁâá8">
                                    <p class="gallery-item-caption">Class-812</p>
                                </div>
                                <div class="gallery-item">
                                    <img src="./assets/visualization/visualization_9.pdf" alt="ÂõæÁâá8">
                                    <p class="gallery-item-caption">Class-980</p>
                                </div><div class="gallery-item">
                                    <img src="./assets/visualization/visualization_10.pdf" alt="ÂõæÁâá8">
                                    <p class="gallery-item-caption">Class-985</p>
                                </div>
                            </div>
                        </div>
                        <button class="gallery-arrow next-arrow" aria-label="‰∏ã‰∏ÄÁªÑÂõæÁâá">&gt;</button>
                    </div>
                </div>
            </div>
        </div>
    </section>
        

    <section class="section fade-in">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content">
                        <p>
                            Large Language Models (LLMs) effectively process variable-length text by training directly on native data formats.
                            This inherent adaptability inspires a critical question for image synthesis: 
                            <b>Can diffusion models achieve similar flexibility, learning to generate images directly at their diverse, native resolutions and aspect ratios?</b> 
                            Conventional diffusion models exhibit significant challenges in generalizing across resolutions beyond their training regime. 
                            This limitation stems from three core difficulties: 
                            (1) Strong coupling between fixed receptive fields in convolutional architectures and learned feature scales.
                            (2) Fragility of positional encoding and spatial coordinate dependencies in transformer architectures.
                            (3) Inefficient and Unstable Training Dynamics from Variable Inputs.
                        </p>

                        <p>
                            We overcome these limitations by proposing a novel architecture for diffusion transformers that directly models native-resolution image data for generation. 
                            Drawing inspiration from the variable-sequence nature of Vision Transformers, we reformulate image generative modeling within diffusion transformers as "native-resolution generation".
                            And we present the Native-resolution diffusion Transformer (NiT), which demonstrates the capability to generate images across a wide spectrum of resolutions and aspect ratios. 
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <h2 class="title is-3">NiT: Native-resolution diffusion Transformer</h2>

                    <div
                        style="background-color: #f8fafc; border-radius: 12px; border-left: 6px solid #047857; padding: 1.2rem; margin-bottom: 1.5rem;">
                        <p style="font-weight: 600; margin-bottom: 0.8rem;">
                            NiT introduces three key architectural innovations
                        </p>

                        <div style="display: flex; flex-direction: column; gap: 0.8rem;">

                            <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: #047857; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                    1
                                </div>
                                <div>
                                    <p style="margin: 0; line-height: 1.4;"><strong>Dynamic Tokenization:</strong>
                                        Converts images in native resolution into variable-length token sequences and the tuples of corresponding height and width. 
                                        Without requiring input padding, it avoids substantial computational overhead.
                                    </p>
                                </div>
                            </div>

                            <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: #047857; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                    2
                                </div>
                                <div>
                                    <p style="margin: 0; line-height: 1.4;"><strong>Variable-Length Sequence Processing:</strong> 
                                        We use Flash Attention to natively process heterogeneous, unpadded token sequences by 
                                        cumulative sequence lengths using the memory tiling strategy.
                                    </p>
                                </div>
                            </div>

                            <div style="display: flex; align-items: flex-start; gap: 0.8rem;">
                                <div
                                    style="background-color: #047857; color: white; padding: 0.4rem; border-radius: 50%; font-size: 0.9rem; display: flex; align-items: center; justify-content: center; min-width: 2rem; height: 2rem;">
                                    3
                                </div>
                                <div>
                                    <p style="margin: 0; line-height: 1.4;"><strong>2D Structural Prior Injection:</strong>
                                        We introduce the axial 2D Rotary Positional Embedding to factorize the height and width impact 
                                        and maximize the 2D structural prior by relative positional encoding. 
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="columns is-centered">
                        <div class="column is-12">
                            <div class="columns is-centered">
                                <div class="column is-12"
                                    style="display: flex; flex-direction: column; justify-content: center;">
                                    <img src="./assets/nit/framework.pdf" alt="Synthetic Data Generation"/>
                                </div>
                            </div>
                            <!-- <p class="figure-caption">
                                <b>Figure 3:</b> Dataset composition showing the diversity of repositories
                                and problems in our synthetic dataset, enabling significantly larger and more diverse
                                training environments than previous methods. Left: Full R2E-Gym dataset. Right:
                                R2E-Gym subset (non-overlapping with SWE-Bench) used for training.
                            </p> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <h2 class="title is-3">Experiments</h2>
                    <h3 class="title is-4">NiT improves training efficiency and generation quality</h3>

                    <div class="columns is-centered">
                        <div class="column is-12">
                            <img src="./assets/nit/in1k_main.png"
                                alt="Distinguishability Plot" />
                            <p class="figure-caption">
                                <b>A single NiT model can compete on both two benchmarks.</b>
                                All these baselines are resolution-expert methods, independently training two models for the two benchmarks. 
                                To the best of our knowledge, this is the first time <b>a single model</b> can compete on these two benchmarks simultaneously.
                                NiT-XL achieves the best FID 1.48 on the 512x512 benchmark, outperforming the previous SOTA EDM2-XXL with half of the model size.
                                On the 256x256 benchmark, our model surpasses the DiT-XL and FiTv2-XL models on FID with the same model size as well as outperforms the LlamaGen-3B model with much smaller parameters.
                            </p>
                        </div>
                    </div>


                    <h3 class="title is-4">NiT improves generalization across diverse resolutions and aspect ratios</h3>

                    <div class="columns is-centered">
                        <div class="column is-12">
                            <img src="./assets/nit/resolution_generalization.png"
                                alt="Distinguishability Plot" />
                            <p class="figure-caption">
                                NiT-XL significantly surpasses all the baselines on resolution generalization. 
                                Remarkably, NiT-XL demonstrates almost no performance degradation when scaling to unseen higher resolutions. 
                            </p>
                        </div>
                    </div>
                    <div class="columns is-centered">
                        <div class="column is-12">
                            <img src="./assets/nit/aspect_ratio_generalization.png"
                                alt="Distinguishability Plot" />
                            <p class="figure-caption">
                                NiT-XL can generalize to arbitrary aspect ratios, greatly outperforming all the baselines.
                            </p>
                        </div>
                    </div>


                    
            </div>
        </div>
    </section>

    


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        If you find our work useful, please cite our paper. BibTex code is provided below:
        <pre><code>
<!-- @article{han2023onellm,
    title={OneLLM: One Framework to Align All Modalities with Language}, 
    author={Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu},
    year={2023},
    eprint={2312.03700},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
} -->
        </code></pre>
        </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
        <div class="container">
            <div class="has-text-centered">
                <p>
                    <strong style="color: white;">Native-Resolution Image Synthesis:</strong>
                </p>
                <p style="font-size: 0.9rem; margin-top: 0.5rem; opacity: 0.8;">
                    Last updated: June 2025
                </p>
            </div>
        </div>
    </footer>

</body>

</html>